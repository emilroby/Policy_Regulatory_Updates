name: Daily Policy Data Commit

on:
  # Schedule the job to run daily at 12:00 PM UTC (You can change the time here)
  schedule:
    - cron: '0 12 * * *' 
  # Allows manual runs from the Actions tab
  workflow_dispatch:

jobs:
  publish_data:
    # Use a fresh, standard Ubuntu environment for running the Python script
    runs-on: ubuntu-latest
    
    # Grant necessary permissions for the workflow to commit files back to the repo
    permissions:
      contents: write # This permission is CRITICAL for committing the JSON file
      
    # Securely map the necessary environment variable
    env:
      # This is the name of the secret you MUST create in GitHub Settings
      REPO_ACCESS_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }}
      # Pass the repository details the Python script needs for API calls
      GITHUB_REPOSITORY_OWNER: ${{ github.repository_owner }}
      GITHUB_REPOSITORY: ${{ github.repository }}

    steps:
      - name: Checkout Repository
        # Action to pull down your code (publish_daily_updates.py, requirements.txt)
        uses: actions/checkout@v4

      - name: Set up Python Environment
        # Action to install Python 3.11 for the job
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        # Install only the required packages (requests, beautifulsoup4)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Policy Scraper and Commit Data
        # This executes your main Python script, which scrapes CTUIL and uses the
        # GITHUB_TOKEN environment variable to commit the new JSON file.
        run: |
          echo "Starting daily policy update run..."
          python publish_daily_updates.py
          echo "Policy data successfully committed to repository."
